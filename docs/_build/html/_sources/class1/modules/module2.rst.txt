Section 2 - F5 Solutions and Technology
=======================================

|

.. raw:: html

   <iframe width="560" height="315" src="https://www.youtube.com/embed/3uDzuRZ47FA?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

|

====

|

Objective - 2.01 Articulate the role of F5 products
---------------------------------------------------

|
|

**2.01 - Explain the purpose, use, and benefits of APM, LTM, ASM, GTM**

`Link to Online Topic Content <http://www.f5.com>`__

**BIG-IP Access Policy Manager (APM)**

Today, business resources, such as applications and data, are accessed
inside and outside the traditional business perimeter. Local and remote
employees, partners, and customers often access applications without
context or security. A central policy control point delivers access
based on context and is critical to managing a scalable, secure, and
dynamic environment.

BIG-IP Access Policy Manager (APM) is a flexible, high-performance
access and security solution that provides unified global access to your
applications and network. By converging and consolidating remote access,
LAN access, and wireless connections within a single management
interface, and providing easy-to-manage access policies, BIG-IP APM
helps you free up valuable IT resources and scale cost-effectively

**Provide unified global access**

BIG-IP APM protects your public-facing applications by providing
policy-based, context-aware access to users while consolidating your
access infrastructure. It also provides secure remote access to
corporate resources from all networks and devices. BIG-IP APM is the
most scalable remote access solution and it is IPv6 ready.

**Obtain flexibility, high performance, and scalability**

BIG-IP APM is available in three deployment options—as an add-on module
for BIG-IP Local Traffic Manager (LTM) for protecting Internet-facing
applications, delivered in BIG-IP Edge Gateway for accelerated remote
access, and run on BIG-IP LTM Virtual Edition to deliver flexible
application access in virtualized environments.

**Consolidate and simplify**

BIG-IP APM helps you consolidate your infrastructure and simplify access
management by providing centralized authentication, authorization, and
accounting (AAA) control directly on the BIG-IP system. BIG-IP APM
integrates with Oracle Access Manager. It simplifies virtual application
deployment by supporting Citrix XenApp and XenDesktop, VMware View,
Microsoft Remote Desktop Protocol (RDP), and Java RDP, all in one
WebTop. With BIG-IP APM, you can consolidate and unify elements such as
access, security, and policy management to help further reduce costs.

**The purpose of the Access Policy Manager is to create a secure access
to internal applications by using a single authentication and provide
control using a single management interface.**

----


`Link to Online Topic Content <http://www.f5.com>`__

**BIG-IP Local Traffic Manager (LTM)**

BIG-IP Local Traffic Manager (LTM) turns your network into an agile
infrastructure for application delivery. It’s a full proxy between users
and application servers, creating a layer of abstraction to secure,
optimize, and load balance application traffic. This gives you the
flexibility and control to add applications and servers easily,
eliminate downtime, improve application performance, and meet your
security requirements.

**Easily deploy applications and ensure availability**

BIG-IP LTM gives you the industry’s most advanced load balancing and
application health monitoring capabilities. F5 iApp enables you to
easily deploy and manage applications using user-defined application
centric configuration templates. The associated application related
statistics provide complete visibility into the health and performance
of your apps for faster troubleshooting and resolution of issues.

**Accelerate your applications up to 3x**

BIG-IP LTM reduces traffic volumes and minimizes the effect of client
connection bottlenecks as well as WAN, LAN, and Internet latency to
improve application and replication performance up to 3x. You can
achieve additional performance increases with BIG-IP Application
Acceleration Manager (see section below).

**Take control over application delivery**

The F5 TMOS platform gives you complete control of the connection,
packets, and payload for applications. Using F5’s event driven iRules
you can customize how you intercept, inspect, transform, and direct
inbound and outbound application traffic. The F5 iControl API makes it
easy to integrate with third-party management systems. Device Service
Clustering provides flexible high-availability scaling and configuration
syncing of live application traffic among a cluster of active or standby
BIG-IP devices

**Reduce servers, bandwidth, and management costs**

Advanced TCP connection management, TCP optimization, and server
offloading enable you to optimize the utilization of your existing
infrastructure—tripling server capacity and reducing bandwidth costs by
up to 80 percent. BIG-IP LTM helps you simplify system management by
consolidating security, acceleration, and availability in one
easy-to-manage platform. By using fewer servers, less bandwidth, less
power, and less cooling, while reducing the time spent managing your
infrastructure, you can significantly reduce your operational costs.

**The purpose of the Local Traffic Manager is to load balance
applications in your environment by using advanced TCP connection
management, TCP optimization and server offloading and also provides a
high security solution. The LTMs iApps functionality is a powerful set
of features that enable you to manage application services rather than
individual devices and objects.**

----


`Link to Online Topic Content <http://www.f5.com>`__

**BIG-IP Application Security Manager (ASM)**

F5 BIG-IP Application Security Manager (ASM) is a flexible web
application firewall that secures web applications in traditional,
virtual, and private cloud environments. BIG-IP ASM provides unmatched
web application and website protection, helps secure deployed
applications against unknown vulnerabilities, and enables compliance for
key regulatory mandates—all on a platform that consolidates application
delivery with data center firewall capabilities, and network and
application access control.

**Deliver comprehensive security**

BIG-IP ASM blocks web application attacks in minutes to help protect
against a broad spectrum of threats, including the latest distributed
denial-of-service (DDoS) and SQL injection attacks. It also helps secure
interactive web applications that use the latest coding, such as AJAX
widgets and JSON payloads. Advanced vulnerability assessment
integrations can scan web applications and BIG-IP ASM patches
vulnerabilities in minutes to help protect against web threats. BIG-IP
ASM stops hackers and attacks from any location and ensures that
legitimate users can access applications.

**The purpose of the Application Security Manager is to secure web
applications using a certified web application firewall and offer threat
assessment and visibility.**

----


`Link to Online Topic Content <http://www.f5.com>`__

**BIG-IP Global Traffic Manager (GTM)**

F5 BIG-IP Global Traffic Manager (GTM) distributes DNS and user
application requests based on business policies, data center and network
conditions, user location, and application performance. BIG-IP GTM
delivers F5’s high-performance DNS Services with visibility, reporting,
and analysis; scales and secures DNS responses geographically to survive
DDoS attacks; delivers a complete, real-time DNSSEC solution; and
ensures global application high availability.

**Control and ensure app availability**

BIG-IP GTM helps you create a strong disaster recovery and business
continuity plan by ensuring that users are always connected to a site
where the application is available. In addition to performing
comprehensive health checking of the entire infrastructure, BIG-IP GTM
minimizes downtime and improves the user experience by determining
health at the application layer for every user.

**Improve application performance**

BIG-IP GTM enables you to send users to a site that will give them the
best application experience. It uses a range of different load balancing
methods and intelligent monitoring for each specific application and
user. Traffic is routed according to your business policies and current
network and user conditions. BIG-IP GTM includes an accurate, granular
geo-location database, giving you control of traffic distribution based
on the user’s location. By providing persistence for stateful
applications, BIG-IP GTM helps you eliminate broken sessions and
corrupted data.

**The purpose of the Global Traffic Manager is to ensure availability
and access to the applications in your environment by using
comprehensive health checks and load balancing methods to determine what
site the user should access to get the best application experience.**

|

----

|

**These products are also in TMOS version 11.4 but not mentioned in the blueprint**

|

`Link to Online Topic
Content <http://www.f5.com/pdf/products/big-ip-application-acceleration-manager-datasheet.pdf>`__

**BIG-IP Application Acceleration Manager (AAM)**

F5 BIG-IP Application Acceleration Manager (AAM) overcomes network,
protocol, and application issues to help you meet application
performance, data replication, and disaster recovery requirements
presented by cloud, mobile applications, and video distribution. By
offloading your network and servers, BIG-IP AAM decreases the need for
additional bandwidth and hardware. Users get fast access to
applications, and you gain greater revenue and free up IT resources for
other strategic projects.

BIG-IP AAM can optimize a wide variety of protocols delivered to a
client browser, a desktop application, or another BIG-IP device,
depending on the deployment. Optimizations are divided into data center
optimizations, including server and network optimizations, transport
optimizations, and application delivery optimizations, including
application protocol and web performance optimizations.

**The purpose of the Application Acceleration Manager is to overcome
WAN latency, maximizes server capacity, and speeds application response
times. AAM decreases the need for additional bandwidth and hardware so
users get fast access to applications, while you gain greater revenue
and free up IT resources.**

----


`Link to Online Topic
Content <http://www.f5.com/pdf/products/big-ip-advanced-firewall-manager-datasheet.pdf>`__

**BIG-IP Advanced Firewall Manager (AFM)**

F5 BIG-IP Advanced Firewall Manager (AFM) is a high-performance,
stateful, full-proxy network firewall designed to guard data centers
against incoming threats that enter the network on the most widely
deployed protocols—including HTTP/S, SMTP, DNS, and FTP. By aligning
firewall policies with the applications they protect, BIG-IP AFM
streamlines application deployment, security, and monitoring. With its
scalability, security, and simplicity, BIG-IP AFM forms the core of the
F5 application delivery firewall solution.

BIG-IP AFM is the core of the F5 application delivery firewall
solution—the first of its kind in the industry, which combines the
network firewall with traffic management, application security, user
access management, and DNS security. By consolidating the security
functions of several BIG-IP modules onto a single platform, the F5
application delivery firewall reduces management complexity and
overhead, while still maintaining superior performance and scalability.
Building upon BIG-IP Local Traffic Manager, the application delivery
firewall has deep application fluency in the most widely deployed
enterprise applications. This makes it ideal for protecting
Internet-facing data center applications, wherever they reside.

**The purpose of the Application Delivery Firewall is to combine the
network firewall with anti-DDoS, traffic management, application
security, user access management, and DNS security. By integrating these
core datacenter features, F5 application delivery firewall reduces
management complexity and overhead and is ideal for protecting
internet-facing data centers wherever they reside.**

|

.. raw:: html

   <iframe width="560" height="315" src="https://www.youtube.com/embed/3uDzuRZ47FA?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

|

====

|

Objective - 2.02 Explain the purpose, use, and advantages of iRules
-------------------------------------------------------------------

|
|

**2.02 - Explain the purpose of iRules**

`Link to Online Topic Content <https://devcentral.f5.com/wiki/iRules.HomePage.ashx>`__

**What is an iRule?**

An iRule is a script that you write if you want to make use of some of
the extended capabilities of the BIG-IP that are unavailable via the CLI
or GUI. iRules allow you to more directly interact with the traffic
passing through the device. Using iRules, you can send traffic not only
to pools, but also to individual pool members, ports, or URIs. And
directing traffic to a desired pool is only the beginning. You can parse
the entire header and payload of the data as it is being passed through
the BIG-IP and, at wire speed, execute an entire script of commands on
that traffic. The commands at your disposal range from logging to
redirecting traffic, from modifying the URI or port to actually
rewriting the payload itself.

The iRules you create can be simple or sophisticated, depending on your
content-switching needs. The following shows an example of a simple
iRule.

This iRule is triggered when a client-side connection has been accepted,
causing the LTM system to send the packet to the pool my\_pool, if the
client's address matches 10.10.10.10.

Using a feature called the Universal Inspection Engine (UIE), you can
write an iRule that searches either a header of a packet, or actual
packet content, and then directs the packet based on the result of that
search. iRules can also direct packets based on the result of a client
authentication attempt.

iRules can direct traffic not only to specific pools, but also to
individual pool members, including port numbers and URI paths, either to
implement persistence or to meet specific load balancing requirements.

The syntax that you use to write iRules is based on the Tool Command
Language (Tcl) programming standard. Thus, you can use many of the
standard Tcl commands, plus a robust set of extensions that the LTM
system provides to help you further increase load balancing efficiency.

The advantage of iRules is that you extend the capabilities of the
BIG-IP that is not available through the CLI or the GUI.

----

|

**2.02 - Explain the advantages of iRules**

`Link to Online Topic Content <https://devcentral.f5.com/articles/-the101-irules-ndash-introduction-to-irules>`__

**How does an iRule work?**

To start at the beginning, as it were, an iRule is first and foremost a
configuration object, in F5 terms. This means that it is a part of your
general bigip.conf along with your pools, virtual servers, monitors,
etc. It is entered into the system either via the GUI or CLI, generally
speaking. There is also an iRules Editor available for download on
DevCentral that is a windows tool for editing and deploying/testing
iRules, which can be extremely useful. Unlike most configuration
objects, though, an iRule is completely user generated and customizable.
An iRule is a script, at its core after all. Regardless of how an iRule
gets there, be it UI, CLI or Editor, once an iRule is part of your
config, it is then compiled as soon as that configuration is saved.

One of the gross misconceptions about iRules is that, as with most
interpreted scripting languages such as TCL, and interpreter must be
instantiated every time an iRule is executed to parse the code and
process it. This is not true at all, because every time you save your
configuration all of your iRules are pre-compiled into what is referred
to as “byte code”. Byte code is mostly compiled and has the vast
majority of the interpreter tasks already performed, so that TMM can
directly interpret the remaining object. This makes for far higher
performance and as such increases scalability.

Now that the iRule is saved and pre-compiled, it must then be applied to
a virtual server before it can affect any traffic. An iRule that is not
applied to a virtual server is effectively disabled, for all intents and
purposes. Once you’ve applied an iRule to a given virtual server,
however, it will now technically be applied against all traffic passing
through that virtual server. Keep in mind though, that this does not
necessarily mean that all traffic passing through the virtual server in
question will be affected. IRules are most often very selective in which
traffic they affect, be it to modify, re-route or otherwise. This is
done through both logical constructs within the iRules, but also through
the use of events within the iRule itself.

Events are one of the ways in which iRules have been made to be network
aware, as a language. An event, which we’ll dig into in much more detail
in the next installment of this series, is a way of executing iRules
code at a given point in time within the flow of a networking session.
If I only want to execute a section of code once for each new connection
to the virtual server to which my iRule is applied, I could easily do so
by writing some simple code in the appropriate event. Events are also
important because they indicate at which point in the proxy chain
(sometimes referred to as a hud chain) an iRule executes. Given that
BIG-IP is a bidirectional proxy, it is important for iRules to execute
on not only the right side of the proxy, but at the right moment in the
network flow.

So now you have an iRule added to your configuration, it has been
automatically pre-compiled to byte code when the configuration was
saved, you have it applied to the appropriate virtual server, and the
code within the iRule calls out the desired event in which you want your
code to execute; now is when the magic happens, as it were. This is
where the massive collection of iRules commands comes into play. From
header modification to full on payload replacement to creating a socket
connection to an outside system and making a request before processing
traffic for your virtual, there are very few limitations to what can be
achieved when combining the appropriate series of iRules commands. Those
commands are then processed by TMM, which will affect whatever change(s)
it needs to the traffic it is processing for the given session,
depending on what you’ve designed your iRule to do. The true power of
iRules largely comes into play thanks to the massive array of custom
commands that we’ve built into the language, allowing you to leverage
your BIG-IP to the fullest.

----

|

**2.02 - Given a list of situations, determine which would be appropriate for the use of iRules**

`Link to Online Topic Content <https://devcentral.f5.com/articles/-the101-irules-ndash-introduction-to-irules>`__

**When would I use an iRule?**

The ideal time to use an iRule is when you’re looking to add some form
of functionality to your application or app deployment, at the network
layer, and that functionality is not already readily available via the
built in configuration options in your BIG-IP. Whether it’s looking to
perform some kind of custom redirect or logging specific information
about users’ sessions or a vast array of other possibilities, iRules can
add valuable business logic or even application functionality to your
deployment. iRules have a single point of management, your BIG-IP, as
opposed to being distributed to every server hosting whichever
application you’re trying to modify or affect. This can save valuable
management time, and can also be a large benefit in time to deployment.
It is often far easier to deploy an iRule or an iRule change than it is
to modify your application for a quick fix.

As an example, one of the most common uses of iRules when it was first
introduced was to redirect all traffic from HTTP (port 80) to HTTPS
(port 443) without affecting either the host or the requested URI for
the connection. (See example below) This was and still is a very simple
iRule, but it wasn’t at the time a feature available in the standard
configuration options for BIG-IP.

when HTTP\_REQUEST {

HTTP::redirect "https://[HTTP::host][HTTP::uri]"

}

**When would I not use an iRule?**

The above example of an HTTP to HTTPS redirect iRule actually depicts
perfectly when to not use an iRule, because that functionality was so
popular that it has since been added as a profile option directly in the
BIG- IP configuration. As such, it is more appropriate and technically
higher performance, to use that feature in the profile as opposed to
writing an iRule to perform the same task. A general rule of thumb is:
Any time you can do something from within the standard config options,
profiles, GUI or CLI - do it there first. If you’re looking to perform a
task that can’t be accomplished via the “built-in” means of
configuration, then it is a perfect time to turn to iRules to expand the
possibilities.

The main reason for this is performance. iRules are extremely high
performance as a rule, if written properly. But there is always a slight
benefit in performance when you can run functionality directly from
built in, core features as opposed to a custom created script, even an
iRule. Also, though, it is easier to maintain a feature built into the
product through upgrades, rather than re-testing and managing an iRule
that could be easily replaced with a few configuration options.

|

.. raw:: html

   <iframe width="560" height="315" src="https://www.youtube.com/embed/3uDzuRZ47FA?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

|

====

|

Objective - 2.03 Explain the purpose, use, and advantages of iApps
------------------------------------------------------------------

|
|

**2.03 - Explain the purpose of iApps**

`Link to Online Topic Content <https://devcentral.f5.com/articles/managing-iapp-template-files-with-icontrol>`__

**What's an iApp?**

An iApp is a user-customizable framework for deploying applications. It
consists of three components: Templates, Application Services, and
Analytics. An iApp Template is where the application is described and
the objects (required and optional) are defined through presentation and
implementation language. An iApp Application Service is the deployment
process of an iApp Template which bundles the entire configuration
options for a particular application together. You would have an iApp
Application Service for SharePoint, for example. iApp Analytics include
performance metrics on a per-application and location basis. Benefits of
using iApp:

-  User-customizable

-  Easy editing of configurations and cleanup

-  Reentrancy

-  Configuration encapsulation

-  Cradle-to-grave configuration management

-  Strictness protects against accidental changes to the configuration

-  Operational tasks and health status for App objects displayed on
   App-specific component view (see right)

-  Copy/Import/Export capability

-  Community support for DevCentral hosted templates

.. image:: /_static/101/1p19.png

----

|

**2.03 - Explain the advantages of iApps**

`Link to Online Topic Content <https://support.f5.com/kb/en-us/products/big-ip_ltm/manuals/product/bigip_iapps_developer_11_0_0/2.html>`__

**About iApp Templates**

iApps templates create configuration-specific forms used by application
services to guide authorized users through complex system
configurations. The templates provide programmatic, visual layout and
help information. Each new application service uses one of the templates
to create a screen with fields and guide the user through the
configuration process and creates the configuration when finished.

The templates allow users to customize by either modifying an existing
template or creating one from scratch. Users can create scratch-built
templates using either the iApps Templates screen or any text-editing
software

The BIG-IP system comes with several system-supplied templates that you
can use as-is to create your application services. You can also use the
system-supplied templates as a starting point for your own templates, or
you can write templates from scratch using, variously, tmsh and TCL for
the back-end template implementation section.

**Template sections**

Templates have three sections; presentation, implementation, and help.

-  The presentation section collects user entries.

-  The implementation section uses user entries to build a configuration
   that will control traffic.

-  The help section documents the template and its presentation to users
   when creating an application service.

.. image:: /_static/101/1p20.png

----

|

**2.03 - Given a list of situations, determine which would be
appropriate for the use of iApps**

`Link to Online Topic Content <https://www.motiv.nl/documenten/whitepapers/f5-iapp-wp>`__

**When do you use an iApp?**

As organizations begin moving to more modular cloud and SaaS models,
managing applications becomes more important than building
infrastructure. Many of the benefits that come from moving to a more
agile model are not associated with managing the infrastructure; yet
managing application deployments, performance, and availability in cloud
and SaaS environments is often difficult because the application is
still tied to infrastructure. iApps bind application control,
visibility, and management to the infrastructure required to deliver
those applications and services beyond the data center.

iApps support the architecture that transforms a network from a static
resource comprising isolated components to a unified, flexible, and
resilient pool of resources directly associated with an application or
service. This enables rapid network deployment, integration, management,
and visibility at the application layer. iApps provide complete control
over the entire application delivery infrastructure by adapting the
network to the application. The resulting quick deployment and
single-point capabilities save operational costs. For the first time,
organizations can create a common and highly reusable catalogue for
security, acceleration, and availability services at a strategic point
of control to dramatically increase the organizational agility and
efficiency of F5 BIG-IP devices.

With iApps, F5 has created a paradigm shift in how administrators view
and manage the network by moving management responsibility from the
network components to the application. iApps increase IT agility and
efficiency by enabling organizations to manage the security,
optimization, availability, health, and performance of not only the ADN
devices in the network, but of the mission-critical applications running
the business. In this way, iApps create a truly unified Application
Delivery Network.

|

.. raw:: html

   <iframe width="560" height="315" src="https://www.youtube.com/embed/3uDzuRZ47FA?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

|

====

|

Objective - 2.04 Explain the purpose of and use cases for full proxy and packet forwarding/packet based architectures
---------------------------------------------------------------------------------------------------------------------

|
|

**2.04 - Describe a full proxy architecture**

`Link to Online Topic Content <https://devcentral.f5.com/Portals/0/Cache/Pdfs/2807/the-concise-guide-to-proxies.pdf>`__

We often mention that the benefits derived from some application
delivery controllers are due to the nature of being a full proxy. And in
the same breath we might mention reverse, half, and forward proxies,
which makes the technology sound more like a description of the
positions on a sports team than an application delivery solution. So
what does these terms really mean? Here's the lowdown on the different
kinds of proxies in one concise guide.

**Proxies**

Proxies (often called intermediaries in the SOA world) are hardware or
software solutions that sit between the client and the server and do
something to requests and sometimes responses. The most often heard use
of the term proxy is in conjunction with making Web surfing anonymous.
That's because proxies sit between your browser and your desired
destination and proxy the connection; that is you talk to the proxy
while the proxy talks to the web server and neither you nor the web
server know about each other.

Proxies are not all the same. Some are half proxies, some are full
proxies; some are forward and some are reverse. Yes, that came
excruciatingly close to sounding like a Dr. Seuss book.

**Forward Proxies**

Forward proxies are probably the most well known of all proxies,
primarily because most folks have dealt with them either directly or
indirectly. Forward proxies are those proxies that sit between two
networks, usually a private internal network and the public Internet.
Large service providers have also traditionally employed forward proxies
as a bridge between their isolated network of subscribers and the public
Internet, such as CompuServe and AOL in days gone by. These are often
referred to as "mega-proxies" because they managed such high volumes of
traffic.

Forward proxies are generally HTTP (Web) proxies that provide a number
of services but primarily focus on web content filtering and caching
services. These forward proxies often include authentication and
authorization as a part of their product to provide more control over
access to public content. If you've ever gotten a web page that says
"Your request has been denied by blah blah. If you think this is an
error please contact the help desk/your administrator" then you've
probably used a forward proxy.

.. image:: /_static/101/1p21.png

----

|

`Link to Online Topic Content <http://www.f5.com>`__

**Reverse Proxies**

A reverse proxy is less well known, generally because we don't use the
term anymore to describe products used as such. Load balancers
(application delivery controllers) and caches are good examples of
reverse proxies. Reverse proxies sit in front of web and application
servers and process requests for applications and content coming in from
the public Internet to the internal, private network. This is the
primary reason for the name "reverse" proxy to differentiate it from a
proxy that handles outbound requests.

Reverse proxies are also generally focused on HTTP but in recent years
have expanded to include a number of other protocols commonly used on
the web such as streaming audio (RTSP), file transfers (FTP), and
generally any application protocol capable of being delivered via UDP or
TCP.

.. image:: /_static/101/1p22.png

----

|

`Link to Online Topic Content <http://www.f5.com>`__

**Half Proxies**

Half-proxy is a description of the way in which a proxy, reverse or
forward, handles connections. There are two uses of the term half-proxy:
one describing a deployment configuration that affects the way
connections are handled and one that describes simply the difference
between a first and subsequent connections.

The deployment-focused definition of half-proxy is associated with a
direct server return (DSR) configuration. Requests are proxied by the
device, but the responses do not return through the device, but rather
are sent directly to the client. For some types of data, particularly
streaming protocols, this configuration results in improved performance.
This configuration is known as a half-proxy, because only half the
connection (incoming) is proxied while the other half, the response, is
not.

.. image:: /_static/101/1p23.png

The second use of the term "half-proxy" describes a solution in which
the proxy performs what is known as delayed binding in order to provide
additional functionality. This allows the proxy to examine the request
before determining where to send it. Once the proxy determines where to
route the request, the connection between the client and the server are
"stitched" together. This is referred to as a half-proxy because the
initial TCP handshaking and first requests are proxied by the solution,
but subsequently forwarded without interception.

.. image:: /_static/101/1p24.png

Half proxies can look at incoming requests in order to determine where
the connection should be sent and can even use techniques to perform
layer 7 inspection, but they are rarely capable of examining the
responses. Almost all half-proxies fall into the category of reverse
proxies.

----

|

`Link to Online Topic Content <http://www.f5.com>`__

**Full Proxies**

Full proxy is also a description of the way in which a proxy, reverse or
forward, handles connections. A full proxy maintains two separate
connections - one between itself and the client and one between itself
and the destination server. A full proxy completely understands the
protocols, and is itself an endpoint and an originator for the
protocols. Full proxies are named because they completely proxy
connections - incoming and outgoing.

Because the full proxy is an actual protocol endpoint, it must fully
implement the protocols as both a client and a server (a packet-based
design does not). This also means the full proxy can have its own TCP
connection behavior, such as buffering, retransmits, and TCP options.
With a full proxy, each connection is unique; each can have its own TCP
connection behavior. This means that a client connecting to the full
proxy device would likely have different connection behavior than the
full proxy might use for communicating with servers. Full proxies can
look at incoming requests and outbound responses and can manipulate both
if the solution allows it.

Many reverse and forward proxies use a full proxy model today. There is
no guarantee that a given solution is a full proxy, so you should always
ask your solution provider if it is important to you that the solution
is a full proxy.

.. image:: /_static/101/1p25.png

----

|

**2.04 - Describe a packet forwarding/packet based architecture**

`Link to Online Topic Content <https://en.wikipedia.org/wiki/Packet_forwarding>`__

**Packet Forwarding**

Packet forwarding is the relaying of packets from one network segment to
another by nodes in a computer network. A unicast forwarding pattern,
typical of many networking technologies including the overwhelming
majority of Internet traffic a multicast-forwarding pattern, typical of
PIM

A broadcast forwarding pattern, typical of bridged Ethernet

The Network Layer of the OSI Layer is responsible for Packet Forwarding.
The simplest forwarding model—​unicasting—​involves a packet being
relayed from link to link along a chain leading from the packet's source
to its destination. However, other forwarding strategies are commonly
used. Broadcasting requires a packet to be duplicated and copies sent on
multiple links with the goal of delivering a copy to every device on the
network. In practice, broadcast packets are not forwarded everywhere on
a network, but only to devices within a broadcast domain, making
broadcast a relative term. Less common than broadcasting, but perhaps of
greater utility and theoretical significance, is multicasting, where a
packet is selectively duplicated and copies delivered to each of a set
of recipients.

Networking technologies tend to naturally support certain forwarding
models. For example, fiber optics and copper cables run directly from
one machine to another to form a natural unicast media - data
transmitted at one end is received by only one machine at the other end.
However, as illustrated in the diagrams, nodes can forward packets to
create multicast or broadcast distributions from naturally unicast
media. Likewise, traditional Ethernet (10BASE5 and 10BASE2, but not the
more modern 10BASE-T) are natural broadcast media - all the nodes are
attached to a single long cable and a packet transmitted by one device
is seen by every other device attached to the cable. Ethernet nodes
implement unicast by ignoring packets not directly addressed to them. A
wireless network is naturally multicast - all devices within a reception
radius of a transmitter can receive its packets. Wireless nodes ignore
packets addressed to other devices, but require forwarding to reach
nodes outside their reception radius.

At nodes where multiple outgoing links are available, the choice of
which, all, or any to use for forwarding a given packet requires a
decision making process that, while simple in concept, is sometimes
bewilderingly complex. Since a forwarding decision must be made for
every packet handled by a node, the total time required for this can
become a major limiting factor in overall network performance. Much of
the design effort of high-speed routers and switches has been focused on
making rapid forwarding decisions for large numbers of packets.

The forwarding decision is generally made using one of two processes:
routing, which uses information encoded in a device's address to infer
its location on the network, or bridging, which makes no assumptions
about where addresses are located and depends heavily on broadcasting to
locate unknown addresses. The heavy overhead of broadcasting has led to
the dominance of routing in large networks, particularly the Internet;
bridging is largely relegated to small networks where the overhead of
broadcasting is tolerable. However, since large networks are usually
composed of many smaller networks linked together, it would be
inaccurate to state that bridging has no use on the Internet; rather,
its use is localized.

A network can use one of two different methods to forward packets:
store-and-forward or cut through.

----

|

`Link to Online Topic Content <https://f5.com/resources/white-papers/tmos-redefining-the-solution>`__

**Packet-based Design**

A network device with a packet-based (or packet-by-packet) design is
located in the middle of a stream of communications, but is not an
endpoint for those communications; it just passes the packets through.
Often a device that operates on a packet-by-packet basis does have some
knowledge of the protocols flowing through it, but is far from being a
real protocol endpoint. The speed of these devices is primarily based on
not having to understand the entire protocol stack, short-cutting the
amount of work needed to handle traffic. For example, with TCP/IP, this
type of device might only understand the protocols well enough to
rewrite the IP addresses and TCP ports; only about half of the entire
stack.

As networks became more complex and the need for intelligence increased,
more advanced packet-based designs began to emerge (including the BIG-IP
products from F5). These devices knew TCP/IP well enough to understand
both TCP connection setup and teardown, modify TCP/IP headers, and even
insert data into TCP streams. Because these systems could insert data
into TCP streams and modify the content of the stream, they also had to
rewrite the TCP sequence (SEQ) and acknowledgment (ACK) values for
packets going back and forth from the client and server. F5’s BIG-IP
products understood TCP/IP and HTTP well enough to identify individual
HTTP requests and could send different requests to different servers,
reusing connections the BIG-IP device already had open.

While all of this is possible using a very sophisticated
packet-by-packet architecture (BIG-IP devices are some of the most
sophisticated of such designs to date), it required a very complex state
tracking engine to understand the TCP/IP and HTTP protocols well enough
to rewrite header contents, insert data, and maintain its own
connections to clients and servers.

Despite this increasing complexity, packet-based designs are still less
complex and faster than traditional proxy-based designs, as they have
the advantage of only requiring a small percentage of the logic required
for a full proxy.

----

|

**2.04 - Given a list of situations, determine which is appropriate for
a full proxy architecture**

`Link to Online Topic Content <https://devcentral.f5.com/articles/the-full-proxy-data-center-architecture>`__

**Full proxy architecture - What do they mean?**

The reason there is a distinction made between “proxy” and “full-proxy”
stems from the handling of connections as they flow through the device.
All proxies sit between two entities - in the Internet age almost always
“client” and “server” - and mediate connections. While all full-proxies
are proxies, the converse is not true. Not all proxies are full-proxies
and it is this distinction that needs to be made when making decisions
that will impact the data center architecture.

A full-proxy maintains two separate session tables - one on the
client-side, one on the server-side. There is effectively an “air gap”
isolation layer between the two internal to the proxy, one that enables
focused profiles to be applied specifically to address issues peculiar
to each “side” of the proxy. Clients often experience higher latency
because of lower bandwidth connections while the servers are generally
low latency because they’re connected via a high-speed LAN. The
optimizations and acceleration techniques used on the client side are
far different than those on the LAN side because the issues that give
rise to performance and availability challenges are vastly different.

.. image:: /_static/101/1p26.png
     :width: 49% 
.. image:: /_static/101/1p27.png
     :width: 49%

A full-proxy, with separate connection handling on either side of the
“air gap”, can address these challenges. A proxy, that may be a
full-proxy but more often than not simply uses a buffer-and-stitch
methodology to perform connection management, cannot optimally do so. A
typical proxy buffers a connection, often through the TCP handshake
process and potentially into the first few packets of application data,
but then “stitches” a connection to a given server on the back-end using
either layer 4 or layer 7 data, perhaps both. The connection is a single
flow from end-to-end and must choose which characteristics of the
connection to focus on - client or server - because it cannot
simultaneously optimize for both.

The second advantage of a full-proxy is its ability to perform more
tasks on the data being exchanged over the connection as it is flowing
through the component. Because specific action must be taken to “match
up” the connection as its flowing through the full-proxy, the component
can inspect, manipulate, and otherwise modify the data before sending it
on its way on the server-side. This is what enables termination of SSL,
enforcement of security policies, and performance-related services to be
applied on a per-client, per-application basis.

This capability translates to broader usage in data center architecture
by enabling the implementation of an application delivery tier in which
operational risk can be addressed through the enforcement of various
policies. In effect, we’re created a full-proxy data center architecture
in which the application delivery tier as a whole serves as the “full
proxy” that mediates between the clients and the applications.

**The full-proxy data center architecture**

A full-proxy data center architecture installs a digital "air gap”
between the client and applications by serving as the aggregation (and
conversely disaggregation) point for services. Because all communication
is funneled through virtualized applications and services at the
application delivery tier, it serves as a strategic point of control at
which delivery policies addressing operational risk (performance,
availability, security) can be enforced.

A full-proxy data center architecture further has the advantage of
isolating end-users from the volatility inherent in highly virtualized
and dynamic environments such as cloud computing. It enables solutions
such as those used to overcome limitations with virtualization
technology, such as those encountered with Pod architectural constraints
in VMware View deployments. Traditional access management technologies,
for example, are tightly coupled to host names and IP addresses. In a
highly virtualized or cloud computing environment, this constraint may
spell disaster for either performance or ability to function, or both.
By implementing access management in the application delivery tier - on
a full-proxy device - volatility is managed through virtualization of
the resources, allowing the application delivery controller to worry
about details such as IP address and VLAN segments, freeing the access
management solution to concern itself with determining whether this user
on this device from that location is allowed to access a given resource.

Basically, we’re taking the concept of a full-proxy and expanded it
outward to the architecture. Inserting an “application delivery tier”
allows for an agile, flexible architecture more supportive of the rapid
changes today’s IT organizations must deal with.

Such a tier also provides an effective means to combat modern attacks.
Because of its ability to isolate applications, services, and even
infrastructure resources, an application delivery tier improves an
organizations’ capability to withstand the onslaught of a concerted DDoS
attack. The magnitude of difference between the connection capacity of
an application delivery controller and most infrastructures (and all
servers) gives the entire architecture a higher resiliency in the face
of overwhelming connections. This ensures better availability and, when
coupled with virtual infrastructure that can scale on-demand when
necessary, can also maintain performance levels required by business
concerns.

A full-proxy data center architecture is an invaluable asset to IT
organizations in meeting the challenges of volatility both inside and
outside the data center.

.. image:: /_static/101/1p28.png

----

|

**2.04 - Given a list of situations, determine which is appropriate for
a packet based architecture**

`Link to Online Topic Content <https://f5.com/resources/white-papers/tmos-redefining-the-solution>`__

**What is a packet-based design?**

A network device with a packet-based (or packet-by-packet) design is
located in the middle of a stream of communications, but is not an
endpoint for those communications; it just passes the packets through.
Often a device that operates on a packet-by-packet basis does have some
knowledge of the protocols flowing through it, but is far from being a
real protocol endpoint. The speed of these devices is primarily based
on not having to understand the entire protocol stack, shortcutting the
amount of work needed to handle traffic. For example, with TCP/IP, this
type of device might only understand the protocols well enough to
rewrite the IP addresses and TCP ports; only about half of the entire
stack.

As networks became more complex and the need for intelligence increased,
more advanced packet-based designs began to emerge (including the BIG-IP
products from F5). These devices knew TCP/IP well enough to understand
both TCP connection setup and teardown, how to modify TCP/IP headers,
and even insert data into TCP streams.

Because these systems could insert data into TCP streams and modify the
content of the stream, they also had to rewrite the TCP sequence (SEQ)
and acknowledgment (ACK) values for packets going back and forth from
the client and server. F5’s BIG-IP products understood TCP/IP and HTTP
well enough to identify individual HTTP requests and could send
different requests to different servers, reusing connections the BIG-IP
device already had open.

While all of this is possible using a very sophisticated
packet-by-packet architecture (BIG-IP devices are some of the most
sophisticated of such designs to date), it required a very complex state
tracking engine to understand the TCP/IP and HTTP protocols well enough
to rewrite header contents, insert data, and maintain its own
connections to clients and servers. Despite this increasing complexity,
packet-based designs are still less complex and faster than traditional
proxy-based designs, as they have the advantage of only requiring a
small percentage of the logic required for a full proxy.

**What is a proxy-based design (full proxy)?**

A full-proxy design is the opposite of a packet-by-packet design.
Instead of having a minimal understanding of the communications
streaming through the device, a full proxy completely understands the
protocols, and is itself an endpoint and an originator for the
protocols. The connection between a client and the full proxy is fully
independent of the connection between the full proxy and the server;
whereas in a packet-by-packet design, there is essentially a direct
communication channel between the client and the server (although the
device in the middle may manipulate the packets going back and forth).

Because the full proxy is an actual protocol endpoint, it must fully
implement the protocols as both a client and a server (a packet-based
design does not). This also means the full proxy can have its own TCP
connection behavior, such as buffering, retransmits, and TCP options.
With a full proxy, each connection is unique; each can have its own TCP
connection behavior. This means that a client connecting to the

full-proxy device would likely have different connection behavior than
the full proxy might use for communicating with the backend servers.

Therefore, a full proxy allows for the optimization of every connection
uniquely, regardless of the original source and the final destination.
Further, a full proxy understands and processes each protocol as a real
client or server would, using layers. Using HTTP as an example, first
the IP protocol is processed, then TCP, then HTTP; and each layer have
no knowledge of the lower layers.

**Redefining the Solution**

It is common knowledge that proxy-based solutions, or at least the
intelligence offered by them, were the ultimate solution. However, the
vastly superior performance of packet-by-packet designs more than made
up for their limited intelligence. For a while, this was an acceptable
trade-off for most enterprise networks. As the need for increased
intelligence grows, packet-based solutions are quickly experiencing the
same performance restrictions that proxy-based solutions have always
suffered from. And the development complexity of packet-based solutions
is quickly approaching that of proxy-based designs as well. Despite
dramatic increases in hardware and software power, packet-by-packet
designs are unable to keep up with the need for intelligence and
performance. It is no longer acceptable to have to choose between them.
While packet-based solutions had their time, that time is gone. It is
now readily apparent that shortcutting intelligence in lieu of
performance did not adequately provide a viable solution. The real
solution is to build a proxy-based solution with the performance of the
packet-based solution.

|

.. raw:: html

   <iframe width="560" height="315" src="https://www.youtube.com/embed/3uDzuRZ47FA?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

|

====

|

Objective - 2.05 Explain the advantages and configurations of high availability (HA)
------------------------------------------------------------------------------------

|
|

**2.05 - F5 High Availability concepts**

`Link to Online Topic Content <https://support.f5.com/kb/en-us/products/big-ip_ltm/manuals/product/tmos_management_guide_10_0_0/tmos_high_avail.html>`__

**Single device**

When you are running the BIG-IP system as a single device (as opposed to
a unit of a redundant system), high availability refers to core services
being up and running on that device, and VLANs being able to send and
receive traffic.

**Redundant devices**

When you are running the BIG-IP system as a unit of a redundant system
configuration, high availability refers to core system services being up
and running on one of the two BIG-IP systems in the configuration. High
availability also refers to a connection being available between the
BIG-IP system and a pool of routers, and VLANs on the system being able
to send and receive traffic.

A redundant system is a type of BIG-IP system configuration that allows
traffic processing to continue in the event that a BIG-IP system becomes
unavailable. A BIG-IP redundant system consists of two identically
configured BIG-IP units. When an event occurs that prevents one of the
BIG-IP units from processing network traffic, the peer unit in the
redundant system immediately begins processing that traffic, and users
experience no interruption in service. 

**What is failover?**

Failover is a process that occurs when one system in a redundant system
becomes unavailable, thereby causing the peer unit to assume the
processing of traffic originally targeted for the unavailable unit. To
facilitate coordination of the failover process, each unit has a unit ID
(1 or 2).

An essential element to making failover successful is a feature called
configuration synchronization. Configuration synchronization, or
ConfigSync, is a process where you replicate one unit’s main
configuration file on the peer unit. Because data is shared in this way,
a unit can process the other unit’s traffic when failover occurs.

By default, the way that a BIG-IP unit monitors the status of its peer,
in order to detect that failover is required, is through a hard-wired
connection between the two BIG-IP units. With proper configuration,
however, you can cause each BIG-IP unit to monitor peer status by way of
a TCP/IP network connection instead.

----

|

**2.05 - Explain active/active**

`Link to Online Topic Content <https://support.f5.com/kb/en-us/solutions/public/15000/000/sol15002.html?sr=46848622>`__

**Understanding active-active redundancy**

Network device failures may occur for a wide variety of reasons,
resulting in unexpected interruptions to applications and/or services.
These unexpected outages may reduce customer satisfaction and
confidence, and as a result, may incur loss of revenue vital for most
organizations that conduct their businesses online. From this
perspective, avoiding outages is critical to these organizations and the
first step to ensure application and service availability is to deploy
devices in an HA configuration. High availability ensures that
applications fail over seamlessly and service continues uninterrupted.
You can also perform troubleshooting while the application or service is
still functioning.

You can deploy BIG-IP systems in an Active-Active configuration for an
HA deployment. Deploying BIG-IP systems in such a manner allows the
active devices in the same cluster to process traffic separately during
normal operations and the ability to failover to one another when
required. However, F5 does not recommend deploying BIG-IP systems in an
Active-Active configuration without a standby device in the cluster, due
to the potential loss of high availability. Consider the following
points:

If each BIG-IP system in the Active-Active configuration can process the
application or services adequately and with some additional resources in
reserve, a failover from one active device to another active device
should be seamless, which allows applications and/or services to
continue uninterrupted.

If each BIG-IP system in the Active-Active configuration is running at
half or greater capacity, a failover from one active device to another
active device may cause the device that is taking over to reach its
processing threshold. As a result, the device fails and interrupts the
applications and/or services. This condition could continue until both
devices are restored to their operational capability.

Starting in BIG-IP 11.0.0, the Device Service Clustering (DSC) feature
allows you to configure more than two BIG-IP systems in an HA
configuration. The DSC feature also allows you to configure multiple
devices as active and one or more devices as standby.

----

|

**2.05 - Explain active/standby**

`Link to Online Topic Content <https://support.f5.com/kb/en-us/products/big-ip_ltm/manuals/product/tmos-implementations-11-4-0/2.html>`__

**Understanding active-standby redundancy**

An active-standby pair is a pair of BIG-IP devices configured so that
one device is actively processing traffic while the other device remains
ready to take over if failover occurs. The two devices synchronize their
configuration data and can fail over to one another in the event that
one of the devices becomes unavailable.

First, you can run the Setup utility on each device to configure base
network components (that is, a management port, administrative
passwords, and the default VLANs and their associated self IP
addresses). Continue running it on each device to establish a trust
relationship between the two devices, and create a Sync-Failover type of
device group that contains two member devices.

After the Setup utility is run on both devices, each device contains the
default traffic group that the BIG-IP system automatically created
during setup. A traffic group represents a set of configuration objects
(such as floating self IP addresses and virtual IP addresses) that
process application traffic. This traffic group actively processes
traffic on one of the two devices, making that device the active device.
When failover occurs, the traffic group will become active on (that is,
float to) the peer BIG-IP device.

By default, the traffic group contains the floating self IP addresses of
the default VLANs. Whenever you create additional configuration objects
such as self IP addresses, virtual IP addresses, and SNATs, the system
automatically adds these objects to the default traffic group.

`Link to Online Topic Content <http://www.f5.com>`__

**Understanding failover in active/standby mode**

When a redundant system is in active/standby mode, one unit is active,
that is, accepting and processing connections on behalf of the redundant
system, while the other unit is idle (that is, in a standby state). When
failover occurs, the standby unit becomes active, and it normally stays
active until failover occurs again, or until you force it into a standby
state. Forcing the unit into a standby state automatically causes the
other system to become active again, if possible.

For example, you can configure unit 1 to process traffic for virtual
servers A and B. The standby unit monitors the active unit, and if
communications fail, the standby unit initiates a failover and becomes
the active unit. The newly-active unit then begins processing traffic
for both virtual servers. You can see an active/standby configuration,
first as it behaves normally, and then after failover has occurred, by
viewing the figure.

.. image:: /_static/101/1p29.png

As you can see in the figure, unit 1 is in an active state, and unit 2
is in a standby state. With this configuration, failover causes the
following to occur:

-  Unit 2 switches to an active state.

-  Unit 2 begins processing the connections that would normally be
   processed by its peer.

When the failed unit becomes available again, you can force a unit to
change its state from active to standby or from standby to active,
thereby initiating failback. Failback on an active/standby system causes
a unit to relinquish any processing that it is doing on behalf of its
peer, and return to a standby state. A redundant system in
active/standby mode is the most common type of redundant system.

|

.. raw:: html

   <iframe width="560" height="315" src="https://www.youtube.com/embed/3uDzuRZ47FA?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

|

====

|